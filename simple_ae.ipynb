{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sos_token, sequence_length, vocab_size, num_layers=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sos_token = sos_token\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze = True)\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "        self.encoder = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder= nn.GRU(self.embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.decoderrr = nn.GRU(latent_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.decoder_cell = nn.GRUCell(self.embedding_dim, latent_dim)\n",
    "        self.fc = nn.Linear(latent_dim, vocab_size)\n",
    "        self.fc_one = nn.Linear(latent_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fcout = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "\n",
    "    def reparametrization(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        #embedded_input = self.layer_norm(embedded_input)\n",
    "        output, hidden = self.encoder(embedded_input)\n",
    "        mu = self.fcmu(hidden)\n",
    "        logvar = self.fclogvar(hidden)\n",
    "\n",
    "        z = self.reparametrization(mu, logvar)\n",
    "        #z = self.hidden_to_latent(hidden)\n",
    "        sos_token = self.sos_token.repeat(x.size(0),1)\n",
    "        #sos_token = self.sos_token.repeat(x.size(0),x.size(1))\n",
    "        sos_token = self.embedding(sos_token)\n",
    "        #sos_token = self.layer_norm(sos_token)\n",
    "        #decoder_inputs = sos_token\n",
    "        decoder_inputs = torch.cat((sos_token, embedded_input), dim = 1)\n",
    "        #decoder_inputs = self.layer_norm(decoder_inputs)\n",
    "        #output, _ = self.decoder(embedded_input, z)\n",
    "\n",
    "        output = []\n",
    "        for t in range(decoder_inputs.shape[1]):\n",
    "            outputs, _ = self.decoder(decoder_inputs[:,t,:].unsqueeze(1), z)\n",
    "            output.append(outputs)\n",
    "        \n",
    "        reconstructed_sequence = torch.cat(output, dim=1)\n",
    "        reconstructed_sequence = self.fc(reconstructed_sequence)\n",
    "\n",
    "        return mu, logvar, reconstructed_sequence[:,1:,:], embedded_input\n",
    "    \n",
    "    def sample(self, x, z):\n",
    "        with torch.no_grad():\n",
    "            embedded_input = self.embedding(x)\n",
    "            #embedded_input = self.layer_norm(embedded_input)\n",
    "            sos_token = self.sos_token.repeat(x.size(0),1)\n",
    "            #sos_token = self.sos_token.repeat(x.size(0),x.size(1))\n",
    "            sos_token = self.embedding(sos_token)\n",
    "            #sos_token = self.layer_norm(sos_token)\n",
    "            #decoder_inputs = sos_token\n",
    "            decoder_inputs = torch.cat((sos_token, embedded_input), dim = 1)\n",
    "            #output, _ = self.decoder(embedded_input, z)\n",
    "\n",
    "            output = []\n",
    "            for t in range(decoder_inputs.shape[1]):\n",
    "                outputs, _ = self.decoder(decoder_inputs[:,t,:].unsqueeze(1), z)\n",
    "                output.append(outputs)\n",
    "\n",
    "            reconstructed_sequence = torch.cat(output, dim = 1)\n",
    "            reconstructed_sequence = self.fc(reconstructed_sequence)\n",
    "\n",
    "        return reconstructed_sequence[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReconVAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sos_token, sequence_length, vocab_size, num_layers=1):\n",
    "        super(ReconVAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.latent_dim = latent_dim\n",
    "        self.sos_token = sos_token\n",
    "        self.sequence_length = sequence_length\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze = True)\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "        self.encoder = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fc_latent_latent = nn.Linear(latent_dim, latent_dim)\n",
    "\n",
    "        self.decoder= nn.GRU(latent_dim, 64, num_layers, batch_first=True)\n",
    "        self.decoderrr = nn.GRU(latent_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.decoder_cell = nn.GRUCell(self.embedding_dim, latent_dim)\n",
    "        self.fc = nn.Linear(64, vocab_size)\n",
    "        self.fc_one = nn.Linear(latent_dim, 1)\n",
    "        self.softmax = nn.Softmax(dim = 2)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fcout = nn.Linear(self.embedding_dim, self.embedding_dim)\n",
    "\n",
    "    def reparametrization(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        #embedded_input = self.layer_norm(embedded_input)\n",
    "        output, hidden = self.encoder(embedded_input)\n",
    "        mu = self.fcmu(hidden[-1])\n",
    "        logvar = self.fclogvar(hidden[-1])\n",
    "\n",
    "        z = self.reparametrization(mu, logvar)\n",
    "\n",
    "        z = self.fc_latent_latent(z)\n",
    "        z = self.relu(z)\n",
    "        z = z.repeat(x.size(1),1).reshape(x.size(0), x.size(1), self.latent_dim)\n",
    "\n",
    "        reconstructed_sequence, _ = self.decoder(z)\n",
    "        reconstructed_sequence = self.fc(reconstructed_sequence)\n",
    "\n",
    "        return mu, logvar, reconstructed_sequence, embedded_input\n",
    "    \n",
    "    def sample(self, z, sequence_length):\n",
    "        with torch.no_grad(): \n",
    "            z = self.fc_latent_latent(z)\n",
    "            z = self.relu(z)\n",
    "            z = z.repeat(sequence_length,1).reshape(z.size(0), sequence_length, self.latent_dim)\n",
    "\n",
    "            reconstructed_sequence, _ = self.decoder(z)\n",
    "            reconstructed_sequence = self.fc(reconstructed_sequence)\n",
    "\n",
    "        return reconstructed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15, 136])"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.randn(size=(64,136))\n",
    "a = a.repeat((15,1))\n",
    "a = a.reshape(64,15,136)\n",
    "a.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAeE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sequence_length, num_layers=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.encoder = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder= nn.GRU(self.embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(latent_dim, self.embedding_dim)\n",
    "\n",
    "    def reparametrization(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        _, hidden = self.encoder(embedded_input)\n",
    "        mu = self.fcmu(hidden)\n",
    "        logvar = self.fclogvar(hidden)\n",
    "\n",
    "        #z = self.reparametrization(mu, logvar)\n",
    "        z = self.hidden_to_latent(hidden)\n",
    "        decoder_inputs = torch.zeros(x.size(0), x.size(1), embedded_input.size(2))\n",
    "        output, _ = self.decoder(decoder_inputs, z)\n",
    "        '''decoder_inputs = embedded_input\n",
    "        output = []\n",
    "        for t in range(x.size(1)):\n",
    "            output_sequence, _ = self.decoder(decoder_inputs[:,t:t+1], z)\n",
    "            #decoder_inputs = output_sequence\n",
    "            output.append(output_sequence)\n",
    "\n",
    "        output = torch.cat(output, dim = 1)'''\n",
    "        reconstructed_sequence = self.fc(output)\n",
    "\n",
    "        return mu, logvar, reconstructed_sequence, embedded_input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usa questo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(size=(32,15,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 14, 300])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input[:,1:].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.view(-1,input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 15, 300])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = torch.FloatTensor(size=(32,136))\n",
    "due = torch.FloatTensor(size=(32,136))\n",
    "tre = torch.stack((uno,due),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 136])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questi sotto non usarli!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, log_var, l_s = 1, loss_fn = nn.MSELoss(), cos_loss = nn.CosineSimilarity(dim=2), CE = nn.CrossEntropyLoss()):\n",
    "    #BCE = loss_fn(recon_x, x)\n",
    "    #BCE = 1 - cos_loss(recon_x,x).mean()\n",
    "    BCE = CE(recon_x.reshape((recon_x.size(0)*recon_x.size(1),recon_x.size(2))),x.view(-1))\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + l_s*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_VAE(vae, train_loader, val_loader, num_epochs, vocab_size, lr = 1e-3):\n",
    "    params = list(vae.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr = lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = 0.0\n",
    "        average_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        average_val_loss = 0.0\n",
    "\n",
    "        for data,_,_ in train_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mu, log_var, reconstructed_data, embedded_input = vae(data)\n",
    "\n",
    "            loss = vae_loss(reconstructed_data, data, mu, log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        average_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f'====> Epoch: {epoch+1} Average loss: {average_loss:.4f}')\n",
    "        train_losses.append(average_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data,_,_ in val_loader:\n",
    "                data = data.to(device)\n",
    "                \n",
    "\n",
    "                mu, log_var, reconstructed_data, embedded_input = vae(data)\n",
    "                \n",
    "                loss = vae_loss(reconstructed_data, data, mu, log_var)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        \n",
    "        average_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "    plt.plot(np.linspace(1,num_epochs,len(train_losses)), train_losses, c = 'darkcyan',label = 'train')\n",
    "    plt.plot(np.linspace(1,num_epochs,len(val_losses)), val_losses, c = 'orange',label = 'val')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(tensor):\n",
    "    bow = torch.zeros(size = (tensor.shape[0],tensor.shape[1]))\n",
    "    #BoW = [(data1[i] == num).sum().item()/data1.shape[1]  for i in range(data1.shape[0]) for num in data1[i] if BoW[i][torch.where(data1[i] == num)[0][0].item()]==0]\n",
    "\n",
    "    for i in range(tensor.shape[0]):\n",
    "        for num in tensor[i]:\n",
    "            index = torch.where(tensor[i] == num)[0][0].item()\n",
    "            bow[i][index] = (tensor[i] == num).sum().item()/tensor.shape[1]\n",
    "\n",
    "    return torch.FloatTensor(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_text(text, sequence_length):\n",
    "    words = text.split()\n",
    "    #words = text\n",
    "    grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),int(sequence_length))]  # range (0,len(words),8)\n",
    "    #grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),2)]\n",
    "    #grouped_words = [words[i] for i in range(0,len(words),19)]\n",
    "    #grouped_words_2d = [sentence.split() for sentence in grouped_words]\n",
    "    output_text = [grouped_words[i].split() for i in range(len(grouped_words)) if len(grouped_words[i].split()) == sequence_length]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dataset(file1 : str,file2 : str, sequence_length, embedding_dim, batch_size, training_fraction):\n",
    "\n",
    "    with open(file1, 'r', encoding='utf-8') as f:\n",
    "        text1 = f.read()\n",
    "\n",
    "\n",
    "    with open(file2, 'r', encoding='utf-8') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    text1 = '<sos> ' + text1\n",
    "    text = text1 + ' ' + text2\n",
    "    divided_text = divide_text(text, sequence_length)\n",
    "\n",
    "    #word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = int(sequence_length/2), min_count=1, workers=4)\n",
    "    word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = 5, min_count=1, workers=4, epochs=50)\n",
    "    #word2vec.train(divided_text, total_examples=word2vec.corpus_count, epochs=20)\n",
    "\n",
    "    # Get the embedding dimension\n",
    "    embedding_dim = word2vec.wv.vector_size\n",
    "\n",
    "    # Prepare the embedding matrix\n",
    "    vocab_size = len(word2vec.wv)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word2idx = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        embedding_matrix[idx] = word2vec.wv[word]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    text1_divided = divide_text(text1, sequence_length)\n",
    "    data1 = torch.LongTensor([[word2idx[char] for char in text1_divided[i]] for i in range(len(text1_divided))])\n",
    "\n",
    "\n",
    "    text2_divided = divide_text(text2, sequence_length)\n",
    "    data2 = torch.LongTensor([[word2idx[char] for char in text2_divided[i]] for i in range(len(text2_divided))])\n",
    "\n",
    "\n",
    "    data1_train = data1[:int(training_fraction * data1.shape[0])]\n",
    "    data1_val = data1[int(training_fraction * data1.shape[0]):]\n",
    "\n",
    "    data2_train = data2[:int(training_fraction * data2.shape[0])]\n",
    "    data2_val = data2[int(training_fraction * data2.shape[0]):]\n",
    "\n",
    "\n",
    "    label0_train = torch.zeros(data1_train.shape[0])\n",
    "    label0_val = torch.zeros(data1_val.shape[0])\n",
    "\n",
    "    label1_train = torch.ones(data2_train.shape[0])\n",
    "    label1_val = torch.ones(data2_val.shape[0])\n",
    "\n",
    "\n",
    "    labels_train = torch.cat((label0_train, label1_train), dim = 0)\n",
    "    labels_val = torch.cat((label0_val, label1_val), dim = 0)\n",
    "\n",
    "    data_train = torch.cat((data1_train, data2_train), dim = 0)\n",
    "    data_val = torch.cat((data1_val, data2_val), dim = 0)\n",
    "\n",
    "    data_train = torch.LongTensor(data_train)\n",
    "    labels_train = labels_train.type(torch.LongTensor)\n",
    "    bow_train = BoW(data_train)\n",
    "\n",
    "    dataset_train = TensorDataset(data_train, bow_train, labels_train)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
    "    #dataloader_train = DataLoader(dataset_train, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    data_val = torch.LongTensor(data_val)\n",
    "    labels_val = labels_val.type(torch.LongTensor)\n",
    "    bow_val = BoW(data_val)\n",
    "\n",
    "    dataset_val = TensorDataset(data_val, bow_val, labels_val)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size = batch_size, shuffle = True)\n",
    "    #dataloader_val = DataLoader(dataset_val, batch_size = batch_size)\n",
    "\n",
    "    return dataloader_train, dataloader_val, embedding_dim, embedding_matrix, word2vec, word2idx, idx2word, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 15\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "latent_dim = 136\n",
    "batch_size = 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train loader:  191\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, embedding_dim, embedding_matrix, word2vec, word2idx, idx2word, vocab_size = custom_dataset('divina_commedia.txt', \n",
    "                                                                                    'uno_nessuno_e_i_malavoglia.txt', \n",
    "                                                                                     sequence_length, \n",
    "                                                                                     embedding_dim,\n",
    "                                                                                     batch_size = batch_size, \n",
    "                                                                                     training_fraction = 0.9)\n",
    "print('len train loader: ', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12762"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.full((1,),word2idx['<sos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = sos_token.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1])"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sos_token.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = sos_token.repeat(64,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "#vae = VAE(embedding_matrix, hidden_dim, latent_dim, sos_token, sequence_length, vocab_size, num_layers = 1)\n",
    "vae = ReconVAE(embedding_matrix, hidden_dim, latent_dim, sos_token, sequence_length, vocab_size, num_layers = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  1801879\n"
     ]
    }
   ],
   "source": [
    "vae_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print('Total parameters: ', vae_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/3 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.2414\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 33%|███▎      | 1/3 [00:18<00:37, 18.82s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 0.1141\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 67%|██████▋   | 2/3 [00:36<00:17, 17.95s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3 Average loss: 0.1120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 3/3 [00:55<00:00, 18.66s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = training_VAE(vae, train_loader, val_loader, 3, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: \n",
      " fiumana uscian faville vive e d ogne parte si mettien ne fiori quasi rubin che\n",
      "\n",
      "Reconstructed sequence: \n",
      " e e e e e e e e e e e e e e e\n",
      "\n",
      "Generated sequence: \n",
      " e e e e e e e e e e e e e e e\n"
     ]
    }
   ],
   "source": [
    "for i ,(data,bow,label) in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        prova = data[0]\n",
    "        labels = label[0]\n",
    "        boww = bow[0]\n",
    "\n",
    "frase = [idx2word[prova[i].item()] for i in range(prova.shape[0])]\n",
    "\n",
    "prova = prova.view(1,prova.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, log_var, reconstructed, embedded_input = vae(prova)\n",
    "\n",
    "indices = torch.argmax(reconstructed, dim=2)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(prova.shape[1]):\n",
    "    ricostruzione.append(idx2word[indices[0][i].item()])\n",
    "\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))\n",
    "\n",
    "z = torch.randn(size=(1,latent_dim))\n",
    "z1 = torch.randint(0,vocab_size,size=(1,15))\n",
    "x = torch.zeros(size=(1,15)).type(torch.LongTensor)\n",
    "x = sos_token.repeat(1,15)\n",
    "nuovo = vae.sample(z, sequence_length)\n",
    "\n",
    "index = nuovo.argmax(dim=-1)\n",
    "\n",
    "generata = []\n",
    "for i in range(prova.shape[1]):\n",
    "    generata.append(idx2word[index[0][i].item()])\n",
    "\n",
    "print(\"\\nGenerated sequence: \\n\", ' '.join(generata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input sequence: \n",
      " di dio per cu io vidi l alto triunfo del regno verace dammi virtù a\n",
      "\n",
      "Reconstructed sequence: \n",
      " l e e e e e e e e e e e e e e\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "repeat(): argument 'repeats' (position 1) must be tuple of ints, but found element of type Tensor at pos 0",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[135], line 27\u001b[0m\n\u001b[1;32m     25\u001b[0m x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros(size\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m15\u001b[39m))\u001b[38;5;241m.\u001b[39mtype(torch\u001b[38;5;241m.\u001b[39mLongTensor)\n\u001b[1;32m     26\u001b[0m x \u001b[38;5;241m=\u001b[39m sos_token\u001b[38;5;241m.\u001b[39mrepeat(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m15\u001b[39m)\n\u001b[0;32m---> 27\u001b[0m nuovo \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msample\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprova\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     29\u001b[0m index \u001b[38;5;241m=\u001b[39m nuovo\u001b[38;5;241m.\u001b[39margmax(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     31\u001b[0m generata \u001b[38;5;241m=\u001b[39m []\n",
      "Cell \u001b[0;32mIn[129], line 49\u001b[0m, in \u001b[0;36mReconVAE.sample\u001b[0;34m(self, z, sequence_length)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21msample\u001b[39m(\u001b[38;5;28mself\u001b[39m, z, sequence_length):\n\u001b[1;32m     48\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad(): \n\u001b[0;32m---> 49\u001b[0m         z \u001b[38;5;241m=\u001b[39m \u001b[43mz\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrepeat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequence_length\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mreshape(z\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), sequence_length, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlatent_dim)\n\u001b[1;32m     51\u001b[0m         reconstructed_sequence, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(z)\n\u001b[1;32m     52\u001b[0m         reconstructed_sequence \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfc(reconstructed_sequence)\n",
      "\u001b[0;31mTypeError\u001b[0m: repeat(): argument 'repeats' (position 1) must be tuple of ints, but found element of type Tensor at pos 0"
     ]
    }
   ],
   "source": [
    "for i ,(data,bow,label) in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        prova = data[0]\n",
    "        labels = label[0]\n",
    "        boww = bow[0]\n",
    "\n",
    "frase = [idx2word[prova[i].item()] for i in range(prova.shape[0])]\n",
    "\n",
    "prova = prova.view(1,prova.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, log_var, reconstructed, embedded_input = vae(prova)\n",
    "\n",
    "indices = torch.argmax(reconstructed, dim=2)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(prova.shape[1]):\n",
    "    ricostruzione.append(idx2word[indices[0][i].item()])\n",
    "\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))\n",
    "\n",
    "z = torch.randn(size=(1,1,latent_dim))\n",
    "z1 = torch.randint(0,vocab_size,size=(1,15))\n",
    "x = torch.zeros(size=(1,15)).type(torch.LongTensor)\n",
    "x = sos_token.repeat(1,15)\n",
    "nuovo = vae.sample(prova, z)\n",
    "\n",
    "index = nuovo.argmax(dim=-1)\n",
    "\n",
    "generata = []\n",
    "for i in range(prova.shape[1]):\n",
    "    generata.append(idx2word[index[0][i].item()])\n",
    "\n",
    "print(\"\\nGenerated sequence: \\n\", ' '.join(generata))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 0, 20249])"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nuovo.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[19999, 15612,   803,  9050,  8297, 13471, 14120, 19550,  8031,  1264,\n",
       "         15653,  6449, 14939,  9195,  9682]])"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "z1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 337,    0,   56, 1547,    1,  103,  330,  269,    0,    4,  131,    5,\n",
       "        1944,   49, 1369])"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 337,    0,   56, 6901,    1,  103,  330,  269,    0,    4,  131,    5,\n",
       "         5569,   49, 5529]])"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prova"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed sequence: \n",
      " tu a la virtù prender la tua misura non a la venendo de le creature\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(size=(1,1,latent_dim))\n",
    "z1 = torch.randint(0,vocab_size,size=(1,15))\n",
    "x = torch.zeros(size=(1,15)).type(torch.LongTensor)\n",
    "x = sos_token.repeat(1,15)\n",
    "nuovo = vae.sample(prova, z)\n",
    "nuovo.shape\n",
    "index = nuovo.argmax(dim=-1)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(prova.shape[1]):\n",
    "    ricostruzione.append(idx2word[index[0][i].item()])\n",
    "\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (300,) (20249,) (300,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 17\u001b[0m\n\u001b[1;32m     15\u001b[0m ricostruzione \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(reconstructed\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m---> 17\u001b[0m     ricostruzione\u001b[38;5;241m.\u001b[39mappend((\u001b[43mword2vec\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmost_similar\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43marray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreconstructed\u001b[49m\u001b[43m[\u001b[49m\u001b[43mi\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43mtopn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;241m0\u001b[39m]))\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n\u001b[1;32m     20\u001b[0m     stile \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDante\u001b[39m\u001b[38;5;124m'\u001b[39m\n",
      "File \u001b[0;32m~/miniconda3/envs/ox/lib/python3.12/site-packages/gensim/models/keyedvectors.py:841\u001b[0m, in \u001b[0;36mKeyedVectors.most_similar\u001b[0;34m(self, positive, negative, topn, clip_start, clip_end, restrict_vocab, indexer)\u001b[0m\n\u001b[1;32m    838\u001b[0m         weight[idx] \u001b[38;5;241m=\u001b[39m item[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m    840\u001b[0m \u001b[38;5;66;03m# compute the weighted average of all keys\u001b[39;00m\n\u001b[0;32m--> 841\u001b[0m mean \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_mean_vector\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeys\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpre_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpost_normalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_missing\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    842\u001b[0m all_keys \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m    843\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_index(key) \u001b[38;5;28;01mfor\u001b[39;00m key \u001b[38;5;129;01min\u001b[39;00m keys \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, _KEY_TYPES) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhas_index_for(key)\n\u001b[1;32m    844\u001b[0m ]\n\u001b[1;32m    846\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m indexer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(topn, \u001b[38;5;28mint\u001b[39m):\n",
      "File \u001b[0;32m~/miniconda3/envs/ox/lib/python3.12/site-packages/gensim/models/keyedvectors.py:511\u001b[0m, in \u001b[0;36mKeyedVectors.get_mean_vector\u001b[0;34m(self, keys, weights, pre_normalize, post_normalize, ignore_missing)\u001b[0m\n\u001b[1;32m    509\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m idx, key \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(keys):\n\u001b[1;32m    510\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, ndarray):\n\u001b[0;32m--> 511\u001b[0m         \u001b[43mmean\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mweights\u001b[49m\u001b[43m[\u001b[49m\u001b[43midx\u001b[49m\u001b[43m]\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\n\u001b[1;32m    512\u001b[0m         total_weight \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;28mabs\u001b[39m(weights[idx])\n\u001b[1;32m    513\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__contains__\u001b[39m(key):\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (300,) (20249,) (300,) "
     ]
    }
   ],
   "source": [
    "for i ,(data,bow,label) in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        prova = data[0]\n",
    "        labels = label[0]\n",
    "        boww = bow[0]\n",
    "\n",
    "frase = [idx2word[prova[i].item()] for i in range(prova.shape[0])]\n",
    "\n",
    "prova = prova.view(1,prova.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, log_var, reconstructed, embedded_input = vae(prova)\n",
    "\n",
    "reconstructed = reconstructed.view(reconstructed.shape[1], reconstructed.shape[2])\n",
    "ricostruzione = []\n",
    "for i in range(reconstructed.shape[0]):\n",
    "    ricostruzione.append((word2vec.wv.most_similar(np.array(reconstructed[i]),topn=1)[0][0]))\n",
    "\n",
    "if labels.item() == 0.0:\n",
    "    stile = 'Dante'\n",
    "else: \n",
    "    stile = 'Italiano'\n",
    "\n",
    "print('Stile: ', stile)\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'VAE' object has no attribute 'decode'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[155], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m z \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m1\u001b[39m, latent_dim)\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 4\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m(z)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(out\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      7\u001b[0m out \u001b[38;5;241m=\u001b[39m out\u001b[38;5;241m.\u001b[39mview(out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m],out\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m~/miniconda3/envs/ox/lib/python3.12/site-packages/torch/nn/modules/module.py:1688\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1686\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1687\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1688\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'VAE' object has no attribute 'decode'"
     ]
    }
   ],
   "source": [
    "z = torch.randn(1,1, latent_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = vae.decode(z)\n",
    "\n",
    "print(out.shape)\n",
    "out = out.view(out.shape[1],out.shape[2])\n",
    "\n",
    "nuova_frase = []\n",
    "for i in range(out.shape[0]):\n",
    "    nuova_frase.append((word2vec.wv.most_similar(np.array(out[i]),topn=1)[0][0]))\n",
    "\n",
    "print(\"\\nNew sequence: \\n\", ' '.join(nuova_frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 300])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 300])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 136])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1,latent_dim).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = VAE.sample(prova,z)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(out.shape[0]):\n",
    "    ricostruzione.append((word2vec.wv.most_similar(np.array(out[i]),topn=1)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed sequence: \n",
      " laggiù all ospedale la nunziata si metteva a piangere anch essa e diceva di no\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
