{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sos_token, vocab_size, num_layers=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.vocab_size = vocab_size\n",
    "        self.sos_token = sos_token\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.encoder = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder= nn.GRU(self.embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(latent_dim, vocab_size)\n",
    "\n",
    "    def reparametrization(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        _, hidden = self.encoder(embedded_input)\n",
    "        mu = self.fcmu(hidden)\n",
    "        logvar = self.fclogvar(hidden)\n",
    "\n",
    "        z = self.reparametrization(mu, logvar)\n",
    "        #z = self.hidden_to_latent(hidden)\n",
    "        decoder_inputs = torch.cat(self.sos_token, embedded_input, dim = 1)\n",
    "        output, _ = self.decoder(decoder_inputs, z)\n",
    "        '''decoder_inputs = embedded_input\n",
    "        output = []\n",
    "        for t in range(x.size(1)):\n",
    "            output_sequence, _ = self.decoder(decoder_inputs[:,t:t+1], z)\n",
    "            #decoder_inputs = output_sequence\n",
    "            output.append(output_sequence)\n",
    "\n",
    "        output = torch.cat(output, dim = 1)'''\n",
    "        reconstructed_sequence = self.fc(output)\n",
    "        reconstructed_sequence = torch.softmax(reconstructed_sequence, dim = 2)\n",
    "\n",
    "        return mu, logvar, reconstructed_sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sequence_length, num_layers=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.encoder = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.hidden_to_latent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.decoder= nn.GRU(self.embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(latent_dim, self.embedding_dim)\n",
    "\n",
    "    def reparametrization(self, mu, logvar):\n",
    "        std = torch.exp(0.5*logvar)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        _, hidden = self.encoder(embedded_input)\n",
    "        mu = self.fcmu(hidden)\n",
    "        logvar = self.fclogvar(hidden)\n",
    "\n",
    "        #z = self.reparametrization(mu, logvar)\n",
    "        z = self.hidden_to_latent(hidden)\n",
    "        decoder_inputs = torch.zeros(x.size(0), x.size(1), embedded_input.size(2))\n",
    "        output, _ = self.decoder(decoder_inputs, z)\n",
    "        '''decoder_inputs = embedded_input\n",
    "        output = []\n",
    "        for t in range(x.size(1)):\n",
    "            output_sequence, _ = self.decoder(decoder_inputs[:,t:t+1], z)\n",
    "            #decoder_inputs = output_sequence\n",
    "            output.append(output_sequence)\n",
    "\n",
    "        output = torch.cat(output, dim = 1)'''\n",
    "        reconstructed_sequence = self.fc(output)\n",
    "\n",
    "        return mu, logvar, reconstructed_sequence, embedded_input\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# usa questo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, num_layers=1):\n",
    "        super(Encoder, self).__init__()\n",
    "        embedding_dim = embedding_matrix.shape[1]\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze=True)\n",
    "        self.gru = nn.GRU(embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu = nn.Linear(hidden_dim, latent_dim)\n",
    "        self.fclogvar = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        _, hidden = self.gru(embedded_input)\n",
    "        mu = self.fcmu(hidden)\n",
    "        logvar = self.fclogvar(hidden)\n",
    "        return mu, logvar, hidden, embedded_input\n",
    "    \n",
    "    def reparametrization(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "        \n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, embedding_dim, hidden_dim, latent_dim, sequence_length, num_layers=1):\n",
    "        super(Decoder, self).__init__()\n",
    "        #self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = nn.GRU(embedding_dim, latent_dim, num_layers, batch_first=True)\n",
    "        self.fc = nn.Linear(latent_dim, embedding_dim)\n",
    "        self.fc_vocab = nn.Linear(latent_dim, sequence_length)\n",
    "\n",
    "    def forward(self, x, hidden):\n",
    "        #x = self.embedding(x)\n",
    "        output, _ = self.gru(x, hidden)\n",
    "        output = self.fc(output)\n",
    "        #output = self.fc_vocab(output)\n",
    "        #output = output.mean(dim=2)\n",
    "        #output = torch.sigmoid(output)\n",
    "        #output = torch.exp(output)\n",
    "        return output\n",
    "    \n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, sequence_length, num_layers=1):\n",
    "        super(VAE, self).__init__()\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.sequence_length = sequence_length\n",
    "        #self.sos_token = self.sos_token.type(torch.FloatTensor)\n",
    "        \n",
    "        self.encoder = Encoder(embedding_matrix, hidden_dim, latent_dim)\n",
    "        self.decoder = Decoder(self.embedding_dim, hidden_dim, latent_dim, sequence_length)\n",
    "        self.latent_to_hidden = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.latent_to_out = nn.Linear(latent_dim, self.embedding_dim)\n",
    "        self.prova1 = nn.Linear(hidden_dim,latent_dim)\n",
    "        self.cell = nn.GRUCell(self.embedding_dim, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        mu, logvar, hidden, embedded_input = self.encoder(x)\n",
    "        z = self.encoder.reparametrization(mu, logvar)\n",
    "        # Use <SOS> token for the initial input to the decoder\n",
    "        #sos_token = torch.FloatTensor([[1]]).repeat(x.size(0), x.size(1), self.embedding_dim).to(x.device)\n",
    "        decoder_inputs = torch.zeros(x.size(0), x.size(1), embedded_input.size(2))\n",
    "        output_sequence = self.decoder(decoder_inputs, z)\n",
    "        \n",
    "        '''output_sequence = []\n",
    "        for t in range(x.size(1)):\n",
    "            outputs = self.decoder(embedded_input[:,t:t+1], z)\n",
    "            output_sequence.append(outputs)\n",
    "            \n",
    "        output_sequence = torch.cat(output_sequence, dim=1)\n",
    "        \n",
    "        for t in range(x.size(1)):\n",
    "            out = self.cell(embedded_input[:,t],z.squeeze(0))\n",
    "            output_sequence.append(out)\n",
    "\n",
    "        output_sequence = torch.stack(output_sequence,dim=1)\n",
    "        output_sequence = self.latent_to_out(output_sequence)\n",
    "        output_sequence = output_sequence.mean(dim=2)\n",
    "        output_sequence = torch.sigmoid(output_sequence)'''\n",
    "        return mu, logvar, output_sequence, embedded_input\n",
    "    \n",
    "    def decode(self, z):\n",
    "        sos_token = self.sos_token.repeat(1, self.sequence_length,1)\n",
    "        output = self.decoder(sos_token, z)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = torch.FloatTensor(size=(32,15,300))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input = input.view(-1,input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([480, 300])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [],
   "source": [
    "uno = torch.FloatTensor(size=(32,136))\n",
    "due = torch.FloatTensor(size=(32,136))\n",
    "tre = torch.stack((uno,due),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([32, 2, 136])"
      ]
     },
     "execution_count": 230,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tre.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# questi sotto non usarli!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu, log_var, l_s = 0.5, loss_fn = nn.MSELoss(), cos_loss = nn.CosineSimilarity(dim=2), CE = nn.CrossEntropyLoss()):\n",
    "    #BCE = loss_fn(recon_x, x)\n",
    "    #BCE = 1 - cos_loss(recon_x,x).mean()\n",
    "    BCE = CE(recon_x.view(-1, recon_x.size(2)),x.view(-1))\n",
    "    KLD = -0.5 * torch.sum(1 + log_var - mu.pow(2) - log_var.exp())\n",
    "    return BCE + l_s*KLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_VAE(vae, train_loader, val_loader, num_epochs, vocab_size, lr = 1e-3):\n",
    "    params = list(vae.parameters())\n",
    "\n",
    "    optimizer = torch.optim.Adam(params, lr = lr)\n",
    "\n",
    "    train_losses = []\n",
    "    val_losses = []\n",
    "\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = 0.0\n",
    "        average_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        average_val_loss = 0.0\n",
    "\n",
    "        for data,_,_ in train_loader:\n",
    "            data = data.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            mu, log_var, reconstructed_data = vae(data)\n",
    "            \n",
    "            loss = vae_loss(reconstructed_data, data, mu, log_var)\n",
    "            loss.backward()\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "        average_loss = train_loss / len(train_loader.dataset)\n",
    "        print(f'====> Epoch: {epoch+1} Average loss: {average_loss:.4f}')\n",
    "        train_losses.append(average_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for data,_,_ in val_loader:\n",
    "                data = data.to(device)\n",
    "                \n",
    "\n",
    "                mu, log_var, reconstructed_data = vae(data)\n",
    "\n",
    "                loss = vae_loss(reconstructed_data, data, mu, log_var)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "        \n",
    "        average_val_loss = val_loss / len(val_loader.dataset)\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "    plt.plot(np.linspace(1,num_epochs,len(train_losses)), train_losses, c = 'darkcyan',label = 'train')\n",
    "    plt.plot(np.linspace(1,num_epochs,len(val_losses)), val_losses, c = 'orange',label = 'val')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return train_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(tensor):\n",
    "    bow = torch.zeros(size = (tensor.shape[0],tensor.shape[1]))\n",
    "    #BoW = [(data1[i] == num).sum().item()/data1.shape[1]  for i in range(data1.shape[0]) for num in data1[i] if BoW[i][torch.where(data1[i] == num)[0][0].item()]==0]\n",
    "\n",
    "    for i in range(tensor.shape[0]):\n",
    "        for num in tensor[i]:\n",
    "            index = torch.where(tensor[i] == num)[0][0].item()\n",
    "            bow[i][index] = (tensor[i] == num).sum().item()/tensor.shape[1]\n",
    "\n",
    "    return torch.FloatTensor(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_text(text, sequence_length):\n",
    "    words = text.split()\n",
    "    #words = text\n",
    "    grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),int(sequence_length/4))]  # range (0,len(words),8)\n",
    "    #grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),2)]\n",
    "    #grouped_words = [words[i] for i in range(0,len(words),19)]\n",
    "    #grouped_words_2d = [sentence.split() for sentence in grouped_words]\n",
    "    output_text = [grouped_words[i].split() for i in range(len(grouped_words)) if len(grouped_words[i].split()) == sequence_length]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dataset(file1 : str,file2 : str, sequence_length, embedding_dim, batch_size, training_fraction):\n",
    "\n",
    "    with open(file1, 'r', encoding='utf-8') as f:\n",
    "        text1 = f.read()\n",
    "\n",
    "\n",
    "    with open(file2, 'r', encoding='utf-8') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    text1 = '<sos> ' + text1\n",
    "    text = text1 + ' ' + text2\n",
    "    divided_text = divide_text(text, sequence_length)\n",
    "\n",
    "    #word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = int(sequence_length/2), min_count=1, workers=4)\n",
    "    word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = 5, min_count=1, workers=4, epochs=50)\n",
    "    #word2vec.train(divided_text, total_examples=word2vec.corpus_count, epochs=20)\n",
    "\n",
    "    # Get the embedding dimension\n",
    "    embedding_dim = word2vec.wv.vector_size\n",
    "\n",
    "    # Prepare the embedding matrix\n",
    "    vocab_size = len(word2vec.wv)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word2idx = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        embedding_matrix[idx] = word2vec.wv[word]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    text1_divided = divide_text(text1, sequence_length)\n",
    "    data1 = torch.LongTensor([[word2idx[char] for char in text1_divided[i]] for i in range(len(text1_divided))])\n",
    "\n",
    "\n",
    "    text2_divided = divide_text(text2, sequence_length)\n",
    "    data2 = torch.LongTensor([[word2idx[char] for char in text2_divided[i]] for i in range(len(text2_divided))])\n",
    "\n",
    "\n",
    "    data1_train = data1[:int(training_fraction * data1.shape[0])]\n",
    "    data1_val = data1[int(training_fraction * data1.shape[0]):]\n",
    "\n",
    "    data2_train = data2[:int(training_fraction * data2.shape[0])]\n",
    "    data2_val = data2[int(training_fraction * data2.shape[0]):]\n",
    "\n",
    "\n",
    "    label0_train = torch.zeros(data1_train.shape[0])\n",
    "    label0_val = torch.zeros(data1_val.shape[0])\n",
    "\n",
    "    label1_train = torch.ones(data2_train.shape[0])\n",
    "    label1_val = torch.ones(data2_val.shape[0])\n",
    "\n",
    "\n",
    "    labels_train = torch.cat((label0_train, label1_train), dim = 0)\n",
    "    labels_val = torch.cat((label0_val, label1_val), dim = 0)\n",
    "\n",
    "    data_train = torch.cat((data1_train, data2_train), dim = 0)\n",
    "    data_val = torch.cat((data1_val, data2_val), dim = 0)\n",
    "\n",
    "    data_train = torch.LongTensor(data_train)\n",
    "    labels_train = labels_train.type(torch.LongTensor)\n",
    "    bow_train = BoW(data_train)\n",
    "\n",
    "    dataset_train = TensorDataset(data_train, bow_train, labels_train)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
    "    #dataloader_train = DataLoader(dataset_train, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    data_val = torch.LongTensor(data_val)\n",
    "    labels_val = labels_val.type(torch.LongTensor)\n",
    "    bow_val = BoW(data_val)\n",
    "\n",
    "    dataset_val = TensorDataset(data_val, bow_val, labels_val)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size = batch_size, shuffle = True)\n",
    "    #dataloader_val = DataLoader(dataset_val, batch_size = batch_size)\n",
    "\n",
    "    return dataloader_train, dataloader_val, embedding_dim, embedding_matrix, word2vec, idx2word, vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 15\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "latent_dim = 136\n",
    "batch_size = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train loader:  1905\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, embedding_dim, embedding_matrix, word2vec, idx2word, vocab_size = custom_dataset('divina_commedia.txt', \n",
    "                                                                                     'divina_commedia.txt', \n",
    "                                                                                     sequence_length, \n",
    "                                                                                     embedding_dim,\n",
    "                                                                                     batch_size = batch_size, \n",
    "                                                                                     training_fraction = 0.9)\n",
    "print('len train loader: ', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12761"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = word2vec.wv['<sos>']\n",
    "word2idx = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "sos_token = torch.FloatTensor(sos_token)\n",
    "#sos_token.shape\n",
    "#idx2word[20249]\n",
    "sos_index = word2idx['<sos>']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.full((32,1),word2idx['<sos>'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = sos_token.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = VAE(embedding_matrix, hidden_dim, latent_dim, sequence_length, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  2460498\n"
     ]
    }
   ],
   "source": [
    "vae_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "print('Total parameters: ', vae_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cat() received an invalid combination of arguments - got (int, Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[46], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m losses \u001b[38;5;241m=\u001b[39m \u001b[43mtraining_VAE\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvocab_size\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 20\u001b[0m, in \u001b[0;36mtraining_VAE\u001b[0;34m(vae, train_loader, val_loader, num_epochs, vocab_size, lr)\u001b[0m\n\u001b[1;32m     16\u001b[0m data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m     18\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 20\u001b[0m mu, log_var, reconstructed_data \u001b[38;5;241m=\u001b[39m \u001b[43mvae\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     22\u001b[0m loss \u001b[38;5;241m=\u001b[39m vae_loss(reconstructed_data, data, mu, log_var)\n\u001b[1;32m     23\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/miniconda3/envs/ox/lib/python3.12/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/ox/lib/python3.12/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[24], line 30\u001b[0m, in \u001b[0;36mVAE.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     28\u001b[0m z \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreparametrization(mu, logvar)\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m#z = self.hidden_to_latent(hidden)\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m decoder_inputs \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msos_token\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43membedded_input\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     31\u001b[0m output, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder(decoder_inputs, z)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m'''decoder_inputs = embedded_input\u001b[39;00m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;124;03moutput = []\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;124;03mfor t in range(x.size(1)):\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     38\u001b[0m \n\u001b[1;32m     39\u001b[0m \u001b[38;5;124;03moutput = torch.cat(output, dim = 1)'''\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: cat() received an invalid combination of arguments - got (int, Tensor, dim=int), but expected one of:\n * (tuple of Tensors tensors, int dim, *, Tensor out)\n * (tuple of Tensors tensors, name dim, *, Tensor out)\n"
     ]
    }
   ],
   "source": [
    "losses = training_VAE(vae, train_loader, val_loader, 5, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stile:  Dante\n",
      "Input sequence: \n",
      " di tal gloria o sodalizio eletto a la gran cena del benedetto agnello il qual\n",
      "\n",
      "Reconstructed sequence: \n",
      " a la sodalizio la sodalizio sodalizio e l sodalizio del del cena cena che che\n"
     ]
    }
   ],
   "source": [
    "for i ,(data,bow,label) in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        prova = data[0]\n",
    "        labels = label[0]\n",
    "        boww = bow[0]\n",
    "\n",
    "frase = [idx2word[prova[i].item()] for i in range(prova.shape[0])]\n",
    "\n",
    "prova = prova.view(1,prova.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    mu, log_var, reconstructed, embedded_input = vae(prova)\n",
    "\n",
    "reconstructed = reconstructed.view(reconstructed.shape[1], reconstructed.shape[2])\n",
    "ricostruzione = []\n",
    "for i in range(reconstructed.shape[0]):\n",
    "    ricostruzione.append((word2vec.wv.most_similar(np.array(reconstructed[i]),topn=1)[0][0]))\n",
    "\n",
    "if labels.item() == 0.0:\n",
    "    stile = 'Dante'\n",
    "else: \n",
    "    stile = 'Italiano'\n",
    "\n",
    "print('Stile: ', stile)\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 15, 300])\n",
      "\n",
      "New sequence: \n",
      " poc che l poc che poc da e l poc poc poc e vèdeisi e\n"
     ]
    }
   ],
   "source": [
    "z = torch.randn(1,1, hidden_dim)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = vae.decode(z)\n",
    "\n",
    "print(out.shape)\n",
    "out = out.view(out.shape[1],out.shape[2])\n",
    "\n",
    "nuova_frase = []\n",
    "for i in range(out.shape[0]):\n",
    "    nuova_frase.append((word2vec.wv.most_similar(np.array(out[i]),topn=1)[0][0]))\n",
    "\n",
    "print(\"\\nNew sequence: \\n\", ' '.join(nuova_frase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([15, 300])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 15, 300])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reconstructed.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 136])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mu.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "z = torch.randn(1,latent_dim).to(device)\n",
    "\n",
    "with torch.no_grad():\n",
    "    out = VAE.sample(prova,z)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(out.shape[0]):\n",
    "    ricostruzione.append((word2vec.wv.most_similar(np.array(out[i]),topn=1)[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reconstructed sequence: \n",
      " laggiù all ospedale la nunziata si metteva a piangere anch essa e diceva di no\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# prove"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, max_sequence_len, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.bilstm1 = nn.LSTM(embed_dim, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)  # 512 because 256 * 2 for bidirectional\n",
    "        self.z_mean = nn.Linear(256, latent_dim)  # 128 * 2 for bidirectional\n",
    "        self.z_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        _, (hn, _) = self.bilstm2(x)\n",
    "        hn = torch.cat((hn[-2], hn[-1]), dim=1)  # Concatenate the final states of both directions\n",
    "        z_mean = self.z_mean(hn)\n",
    "        z_log_var = self.z_log_var(hn)\n",
    "        return z_mean, z_log_var\n",
    "    \n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, output_dim, sequence_length):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense = nn.Linear(latent_dim, 128)\n",
    "        self.repeat = nn.Linear(128, sequence_length * 64)  # Dense layer followed by reshape\n",
    "        self.lstm = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.output_dense = nn.Linear(64, output_dim)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.repeat(x)\n",
    "        x = x.view(-1, sequence_length, 64)  # Reshape to (batch_size, max_sequence_len, 64)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.output_dense(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reparameterize(z_mean, z_log_var):\n",
    "    std = torch.exp(0.5 * z_log_var)\n",
    "    epsilon = torch.randn_like(std)\n",
    "    return z_mean + std * epsilon"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, max_sequence_len, latent_dim, output_dim):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, embed_dim, max_sequence_len, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, output_dim, max_sequence_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var = self.encoder(x)\n",
    "        z = reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(z)\n",
    "        return z_mean, z_log_var, decoded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_mean shape: torch.Size([32, 50])\n",
      "decoded shape: torch.Size([64, 15, 10000])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10000  # Vocabulary size or number of unique tokens\n",
    "embed_dim = 128    # Dimension of embeddings\n",
    "max_sequence_len = 30  # Length of sequences\n",
    "latent_dim = 50    # Size of the latent vector\n",
    "output_dim = 10000  # Output dimension (same as input_dim for token probabilities)\n",
    "\n",
    "# Create the model\n",
    "vae = VAE(input_dim, embed_dim, max_sequence_len, latent_dim, output_dim)\n",
    "\n",
    "# Example input: batch_size=32, sequence length=30\n",
    "example_input = torch.randint(0, input_dim, (32, max_sequence_len))\n",
    "\n",
    "# Forward pass\n",
    "z_mean, z_log_var, decoded = vae(example_input)\n",
    "\n",
    "print(\"z_mean shape:\", z_mean.shape)  # Should be (32, latent_dim)\n",
    "print(\"decoded shape:\", decoded.shape)  # Should be (32, max_sequence_len, output_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, max_sequence_len, latent_dim):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(input_dim, embed_dim)\n",
    "        self.bilstm1 = nn.LSTM(embed_dim, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout = nn.Dropout(0.15)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)  # 512 because 256 * 2 for bidirectional\n",
    "        self.z_mean = nn.Linear(256, latent_dim)  # 128 * 2 for bidirectional\n",
    "        self.z_log_var = nn.Linear(256, latent_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout(x)\n",
    "        _, (hn, _) = self.bilstm2(x)\n",
    "        h_n = torch.cat((hn[-2], hn[-1]), dim=1)  # Concatenate the final states of both directions\n",
    "        z_mean = self.z_mean(h_n)\n",
    "        z_log_var = self.z_log_var(h_n)\n",
    "        return z_mean, z_log_var, hn\n",
    "\n",
    "def reparameterize(z_mean, z_log_var):\n",
    "    std = torch.exp(0.5 * z_log_var)\n",
    "    epsilon = torch.randn_like(std)\n",
    "    return z_mean + std * epsilon\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, latent_dim, vocab_size, max_sequence_len):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dense = nn.Linear(latent_dim, 128)\n",
    "        self.repeat_vector = nn.Linear(128, max_sequence_len * 64)\n",
    "        self.lstm = nn.LSTM(64, 64, batch_first=True)\n",
    "        self.output_dense = nn.Linear(64, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dense(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.repeat_vector(x)\n",
    "        x = x.view(-1, max_sequence_len, 64)  # Reshape to (batch_size, max_sequence_len, 64)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = self.output_dense(x)\n",
    "        return x\n",
    "\n",
    "class VAE(nn.Module):\n",
    "    def __init__(self, input_dim, embed_dim, max_sequence_len, latent_dim, vocab_size):\n",
    "        super(VAE, self).__init__()\n",
    "        self.encoder = Encoder(input_dim, embed_dim, max_sequence_len, latent_dim)\n",
    "        self.decoder = Decoder(latent_dim, vocab_size, max_sequence_len)\n",
    "\n",
    "    def forward(self, x):\n",
    "        z_mean, z_log_var, hn = self.encoder(x)\n",
    "        z = reparameterize(z_mean, z_log_var)\n",
    "        decoded = self.decoder(z)\n",
    "        return z_mean, z_log_var, decoded, hn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "z_mean shape: torch.Size([64, 50])\n",
      "decoded shape: torch.Size([64, 15, 10000])\n"
     ]
    }
   ],
   "source": [
    "input_dim = 10000  # Vocabulary size or number of unique tokens\n",
    "embed_dim = 128    # Dimension of embeddings\n",
    "max_sequence_len = 15  # Length of sequences\n",
    "latent_dim = 50    # Size of the latent vector\n",
    "vocab_size = 10000  # Output dimension (same as input_dim for token probabilities)\n",
    "\n",
    "# Create the model\n",
    "vae = VAE(input_dim, embed_dim, max_sequence_len, latent_dim, vocab_size)\n",
    "\n",
    "# Example input: batch_size=64, sequence length=15\n",
    "example_input = torch.randint(0, input_dim, (64, max_sequence_len))\n",
    "\n",
    "# Forward pass\n",
    "z_mean, z_log_var, decoded , hn = vae(example_input)\n",
    "\n",
    "print(\"z_mean shape:\", z_mean.shape)  # Should be (64, latent_dim)\n",
    "print(\"decoded shape:\", decoded.shape)  # Should be (64, max_sequence_len, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 15])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "example_input.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 64, 128])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "s = torch.cat((hn[-2], hn[-1]), dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 256])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 128])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hn[-2].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
