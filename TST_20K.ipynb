{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.utils.data import DataLoader,TensorDataset\n",
    "from gensim.models import Word2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRUVAE(nn.Module):\n",
    "    def __init__(self, embedding_matrix, hidden_dim, latent_dim, vocab_size, sos_token, num_layers):\n",
    "        super(GRUVAE, self).__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_matrix.shape[1]\n",
    "        self.sos_token = sos_token\n",
    "\n",
    "        self.embedding = nn.Embedding.from_pretrained(embedding_matrix, freeze = True)\n",
    "        self.layer_norm = nn.LayerNorm(self.embedding_dim)\n",
    "        self.encoder_style = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True) # (N,B,H) N batches, B sequence length, H input dim\n",
    "        self.encoder_content = nn.GRU(self.embedding_dim, hidden_dim, num_layers, batch_first=True)\n",
    "        self.fcmu_s = nn.Linear(hidden_dim, 8)\n",
    "        self.fcvar_s = nn.Linear(hidden_dim, 8)\n",
    "        self.fcmu_c = nn.Linear(hidden_dim, 128)\n",
    "        self.fcvar_c = nn.Linear(hidden_dim, 128)\n",
    "\n",
    "        self.fc_s = nn.Linear(hidden_dim, 8)\n",
    "        self.fc_c = nn.Linear(hidden_dim, 128)\n",
    "        self.fclatent = nn.Linear(hidden_dim, latent_dim)\n",
    "\n",
    "        self.fc = nn.Linear(latent_dim, hidden_dim)\n",
    "        self.decoder = nn.GRU(self.embedding_dim, latent_dim, num_layers,batch_first=True)\n",
    "        self.fc_out = nn.Linear(latent_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded_input = self.embedding(x)\n",
    "        #out = self.layer_norm(embedded_input)\n",
    "        _, hn = self.encoder_style(embedded_input)\n",
    "        #_, hn_c = self.encoder_content(out)\n",
    "        mu_s = self.fcmu_s(hn)\n",
    "        logvar_s = self.fcvar_s(hn)\n",
    "        mu_c = self.fcmu_c(hn)\n",
    "        logvar_c = self.fcvar_c(hn)\n",
    "\n",
    "        style = self.reparametrization(mu_s, logvar_s)\n",
    "        content = self.reparametrization(mu_c, logvar_c)\n",
    "        z = torch.cat((style,content), dim = 2)\n",
    "\n",
    "        sos_token = self.sos_token.repeat(x.size(0),1)\n",
    "        sos_token = self.embedding(sos_token)\n",
    "        decoder_input = torch.cat((sos_token, embedded_input), dim = 1)\n",
    "\n",
    "        output = []\n",
    "        for t in range(decoder_input.shape[1]):\n",
    "            outputs, _ = self.decoder(decoder_input[:,t,:].unsqueeze(1), z)\n",
    "            output.append(outputs)\n",
    "        \n",
    "        reconstructed_sequence = torch.cat(output, dim=1)\n",
    "        reconstructed_sequence = self.fc_out(reconstructed_sequence)\n",
    "\n",
    "        return reconstructed_sequence[:,1:,:], style, content, mu_s, logvar_s, mu_c, logvar_c\n",
    "\n",
    "\n",
    "    def reparametrization(self, mu, log_var):\n",
    "        std = torch.exp(0.5*log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps*std\n",
    "    \n",
    "    def style_transfer(self, x, new_style):\n",
    "        embedded_input = self.embedding(x)\n",
    "        #out = self.layer_norm(embedded_input)\n",
    "        _, hn = self.encoder_style(embedded_input)\n",
    "\n",
    "        mu_c = self.fcmu_c(hn)\n",
    "        log_var_c = self.fcvar_c(hn)\n",
    "\n",
    "        content = self.reparametrization(mu_c,log_var_c)\n",
    "\n",
    "        z = torch.cat((new_style,content), dim = 2)\n",
    "\n",
    "        sos_token = self.sos_token.repeat(x.size(0),1)\n",
    "        sos_token = self.embedding(sos_token)\n",
    "        decoder_input = torch.cat((sos_token, embedded_input), dim = 1)\n",
    "\n",
    "        output = []\n",
    "        for t in range(decoder_input.shape[1]):\n",
    "            outputs, _ = self.decoder(decoder_input[:,t,:].unsqueeze(1), z)\n",
    "            output.append(outputs)\n",
    "        \n",
    "        reconstructed_sequence = torch.cat(output, dim=1)\n",
    "        reconstructed_sequence = self.fc_out(reconstructed_sequence)\n",
    "\n",
    "        return reconstructed_sequence[:,1:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class StyleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(StyleClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp = nn.Sequential(nn.Linear(input_dim,int(input_dim*0.5)),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(int(input_dim*0.5),1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.mlp(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out.view(out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvStyleClassifier(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super(AdvStyleClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.mlp = nn.Sequential(nn.Linear(input_dim,int(input_dim*0.5)),\n",
    "                                  nn.ReLU(),\n",
    "                                  nn.Linear(int(input_dim*0.5),1))\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.mlp(x)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out.view(out.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, vocab_size):\n",
    "        super(ContentClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(input_dim,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.fc(x)\n",
    "        out = F.softmax(out,dim=2)\n",
    "        return out.squeeze(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdvContentClassifier(nn.Module):\n",
    "    def __init__(self, input_dim, vocab_size):\n",
    "        super(AdvContentClassifier, self).__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.vocab_size = vocab_size\n",
    "        self.fc = nn.Linear(input_dim,vocab_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "        out = self.fc(x)\n",
    "        out = F.softmax(out,dim=2)\n",
    "        return out.squeeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vae_loss(recon_x, x, mu_s, logvar_s, mu_c, logvar_c, l_s = 0.05, l_c = 0.05, loss_fn = nn.MSELoss(), cos = nn.CosineSimilarity(), CE = nn.CrossEntropyLoss()):\n",
    "    #BCE = loss_fn(recon_x, x)\n",
    "    #BCE = 1 - cos(recon_x,x).mean()\n",
    "    BCE = CE(recon_x.reshape((recon_x.size(0)*recon_x.size(1),recon_x.size(2))),x.view(-1))\n",
    "    KLD_s = -0.5 * torch.sum(1 + logvar_s - mu_s.pow(2) - logvar_s.exp())\n",
    "    KLD_c = -0.5 * torch.sum(1 + logvar_c - mu_c.pow(2) - logvar_c.exp())\n",
    "    return 0.8*BCE + l_s*KLD_s + l_c*KLD_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_s_loss(y_s, labels, loss_fn=nn.BCELoss()):\n",
    "    L_mul_s = loss_fn(y_s, labels)\n",
    "\n",
    "    return L_mul_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mul_c_loss(y_c, bow, loss_fn=nn.BCELoss()):\n",
    "    L_mul_c = loss_fn(y_c, bow)\n",
    "\n",
    "    return L_mul_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_s_loss(y_s, labels, loss_fn=nn.BCELoss()):\n",
    "    L_dis_s = loss_fn(y_s, labels)\n",
    "\n",
    "    return L_dis_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dis_c_loss(y_c, bow, loss_fn=nn.BCELoss()):\n",
    "    L_dis_c = loss_fn(y_c, bow)\n",
    "\n",
    "    return L_dis_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_s_loss(y_s, loss_fn=nn.BCELoss()):\n",
    "    L_adv_s = loss_fn(y_s, y_s)\n",
    "\n",
    "    return L_adv_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def adv_c_loss(y_c, loss_fn = nn.BCELoss()):\n",
    "    L_adv_c = loss_fn(y_c,y_c)\n",
    "\n",
    "    return L_adv_c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def total_loss(recon_x, x, mu_s, logvar_s, mu_c, logvar_c, y_s, y_c, y_s_given_c, y_c_given_s, labels, bow, l_muls=10, l_mulc=3, l_advs=1, l_advc=0.03):\n",
    "    L_VAE = vae_loss(recon_x, x, mu_s, logvar_s, mu_c, logvar_c)\n",
    "    L_muls = mul_s_loss(y_s, labels)\n",
    "    L_mulc = mul_c_loss(y_c, bow)\n",
    "    L_advs = adv_s_loss(y_s_given_c)\n",
    "    L_advc = adv_c_loss(y_c_given_s)\n",
    "\n",
    "    return L_VAE + l_muls*L_muls + l_mulc*L_mulc - l_advs*L_advs - l_advc*L_advc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_VAE(vae, style_classif, adv_style_classif, content_classif, adv_content_classif, train_loader, val_loader, num_epochs, vocab_size, lr = 4e-4):\n",
    "    params_tot = list(vae.parameters()) + list(style_classif.parameters()) + list(content_classif.parameters())\n",
    "    params_dis_s = list(adv_style_classif.parameters())\n",
    "    params_dis_c = list(adv_content_classif.parameters())\n",
    "\n",
    "    optimizer_tot = torch.optim.Adam(params_tot, lr = lr)\n",
    "    optimizer_dis_s = torch.optim.Adam(params_dis_s, lr = lr)\n",
    "    optimizer_dis_c = torch.optim.Adam(params_dis_c, lr = lr)\n",
    "\n",
    "    average_losses = []\n",
    "    val_losses = []\n",
    "    \n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        train_loss = 0.0\n",
    "        average_loss = 0.0\n",
    "        val_loss = 0.0\n",
    "        average_val_loss = 0.0\n",
    "        \n",
    "        #model.train()\n",
    "        for  i, (data, bow, labels) in enumerate(train_loader):\n",
    "            data = data.to(device)\n",
    "            labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "            optimizer_tot.zero_grad()\n",
    "            optimizer_dis_s.zero_grad()\n",
    "            optimizer_dis_c.zero_grad()\n",
    "\n",
    "            reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c = vae(data)\n",
    "            \n",
    "        \n",
    "            predicted_adv_style = adv_style_classif(content)\n",
    "            predicted_adv_style = predicted_adv_style.type(torch.FloatTensor)\n",
    "        \n",
    "    \n",
    "            L_dis_s = dis_s_loss(predicted_adv_style, labels)\n",
    "\n",
    "            L_dis_s.backward()\n",
    "            optimizer_dis_s.step()\n",
    "\n",
    "            reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c = vae(data)\n",
    "\n",
    "\n",
    "            predicted_adv_content = adv_content_classif(style)\n",
    "\n",
    "            L_dis_c = dis_c_loss(predicted_adv_content, bow)\n",
    "\n",
    "            L_dis_c.backward()\n",
    "            optimizer_dis_c.step()\n",
    "\n",
    "\n",
    "            reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c = vae(data)\n",
    "            \n",
    "\n",
    "            y_s = style_classif(style)\n",
    "            y_c = content_classif(content)\n",
    "            y_s_given_c = adv_style_classif(content)\n",
    "            y_c_given_s = adv_content_classif(style)\n",
    "\n",
    "            reconstructed_sequence = torch.FloatTensor(reconstructed_sequence)\n",
    "            \n",
    "            #data = data.type(torch.FloatTensor)\n",
    "            loss_tot = total_loss(reconstructed_sequence, data, mu_s, logvar_s, mu_c, logvar_c, y_s, y_c, y_s_given_c, y_c_given_s, labels, bow)\n",
    "            loss_tot.backward()\n",
    "            train_loss += loss_tot.item()\n",
    "\n",
    "\n",
    "            optimizer_tot.step()\n",
    "            \n",
    "            if (i + 1) % 5000 == 0:\n",
    "                print(f'Train Epoch: {epoch+1} [{i * len(data)}/{len(train_loader.dataset)} ({100. * i / len(train_loader):.0f}%)]\\tLoss: {loss_tot.item() / len(data):.6f}')\n",
    "        \n",
    "        \n",
    "        average_loss = train_loss / len(train_loader.dataset)\n",
    "        #plt.plot(epoch+1,average_loss)\n",
    "        print(f'====> Epoch: {epoch+1} Average loss: {average_loss:.4f}')\n",
    "        average_losses.append(average_loss)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, (data, bow, labels) in enumerate(val_loader):\n",
    "                data = data.to(device)\n",
    "                labels = labels.type(torch.FloatTensor)\n",
    "\n",
    "                reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c = vae(data)\n",
    "                \n",
    "\n",
    "                y_s = style_classif(style)\n",
    "                y_c = content_classif(content)\n",
    "                y_s_given_c = adv_style_classif(content)\n",
    "                y_c_given_s = adv_content_classif(style)\n",
    "\n",
    "                reconstructed_sequence = torch.FloatTensor(reconstructed_sequence)\n",
    "                \n",
    "                #data = data.type(torch.FloatTensor)\n",
    "                val_loss_tot = total_loss(reconstructed_sequence, data, mu_s, logvar_s, mu_c, logvar_c, y_s, y_c, y_s_given_c, y_c_given_s, labels, bow)\n",
    "                val_loss += val_loss_tot.item()\n",
    "\n",
    "\n",
    "                \n",
    "                if (i + 1) % 5000 == 0:\n",
    "                    print(f'Train Epoch: {epoch+1} [{i * len(data)}/{len(val_loader.dataset)} ({100. * i / len(val_loader):.0f}%)]\\tLoss: {val_loss_tot.item() / len(data):.6f}')\n",
    "            \n",
    "            \n",
    "        average_val_loss = val_loss / len(val_loader.dataset)\n",
    "        #print(f'====> Epoch: {epoch+1} Average loss: {average_val_loss:.4f}')\n",
    "        val_losses.append(average_val_loss)\n",
    "\n",
    "    \n",
    "    plt.plot(np.linspace(1,num_epochs,len(average_losses)), average_losses, c = 'darkcyan',label = 'train')\n",
    "    plt.plot(np.linspace(1,num_epochs,len(val_losses)), val_losses, c = 'orange',label = 'val')\n",
    "    plt.legend()\n",
    "    plt.xlabel(\"Epochs\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.show()\n",
    "    return average_losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# è importantissimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nomeFile='divina_commedia.txt'\n",
    "nomeFile='lo_cunto_de_li_cunti.txt'\n",
    "#nomeFile = 'uno_nessuno_e_i_malavoglia.txt'\n",
    "#nomeFile = 'tutto_assieme.txt'\n",
    "\n",
    "with open(nomeFile, 'r', encoding='utf-8') as f:\n",
    "    text = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49925\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['antuono',\n",
       " 'de',\n",
       " 'marigliano',\n",
       " 'ped',\n",
       " 'essere',\n",
       " 'l',\n",
       " 'arcenfanfaro',\n",
       " 'de',\n",
       " 'li',\n",
       " 'catammare']"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(len(text.split()))\n",
    "text.split()[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def BoW(tensor, vocab_size, sequence_length):\n",
    "    bow = torch.zeros(size = (tensor.shape[0],vocab_size))\n",
    "    #BoW = [(data1[i] == num).sum().item()/data1.shape[1]  for i in range(data1.shape[0]) for num in data1[i] if BoW[i][torch.where(data1[i] == num)[0][0].item()]==0]\n",
    "\n",
    "    for i in range(tensor.shape[0]):\n",
    "        for num in tensor[i]:\n",
    "            bow[i][num] = (tensor[i] == num).sum().item()/sequence_length\n",
    "\n",
    "    return torch.FloatTensor(bow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_text(text, sequence_length):\n",
    "    words = text.split()\n",
    "    #words = text\n",
    "    grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),int(sequence_length/2))]  # range (0,len(words),8)\n",
    "    #grouped_words = [' '.join(words[i:i+sequence_length]) for i in range(0,len(words),2)]\n",
    "    #grouped_words = [words[i] for i in range(0,len(words),19)]\n",
    "    #grouped_words_2d = [sentence.split() for sentence in grouped_words]\n",
    "    output_text = [grouped_words[i].split() for i in range(len(grouped_words)) if len(grouped_words[i].split()) == sequence_length]\n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['antuono',\n",
       " 'de',\n",
       " 'marigliano',\n",
       " 'ped',\n",
       " 'essere',\n",
       " 'l',\n",
       " 'arcenfanfaro',\n",
       " 'de',\n",
       " 'li',\n",
       " 'catammare',\n",
       " 'cacciato',\n",
       " 'da',\n",
       " 'la',\n",
       " 'mamma',\n",
       " 'se',\n",
       " 'mese',\n",
       " 'a',\n",
       " 'li',\n",
       " 'servizie']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_divided = divide_text(text, 19)\n",
    "np.shape(text_divided)\n",
    "text_divided[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2vec = Word2Vec(text_divided, vector_size = 300, window = 5, min_count=1, workers=4, epochs = 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "word2idx = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8062"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word2idx['sieste']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(text_divided)):\n",
    "    for char in text_divided[i]:\n",
    "        if char == 'sieste':\n",
    "            print('sieste?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_dataset(file1 : str,file2 : str, sequence_length, embedding_dim, batch_size, training_fraction):\n",
    "\n",
    "    with open(file1, 'r', encoding='utf-8') as f:\n",
    "        text1 = f.read()\n",
    "\n",
    "\n",
    "    with open(file2, 'r', encoding='utf-8') as f:\n",
    "        text2 = f.read()\n",
    "\n",
    "    text1 = '<sos> ' + text1 \n",
    "    text = text1 + ' ' + text2\n",
    "    divided_text = divide_text(text, sequence_length)\n",
    "\n",
    "    #word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = int(sequence_length/2), min_count=1, workers=4)\n",
    "    word2vec = Word2Vec(divided_text, vector_size = embedding_dim, window = 5, min_count=1, workers=4, epochs = 50)\n",
    "    #word2vec.train(divided_text, total_examples=word2vec.corpus_count, epochs=20)\n",
    "\n",
    "    # Get the embedding dimension\n",
    "    embedding_dim = word2vec.wv.vector_size\n",
    "\n",
    "    # Prepare the embedding matrix\n",
    "    vocab_size = len(word2vec.wv)\n",
    "    embedding_matrix = np.zeros((vocab_size, embedding_dim))\n",
    "    word2idx = {word: idx for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "    idx2word = {idx: word for idx, word in enumerate(word2vec.wv.index_to_key)}\n",
    "\n",
    "    for word, idx in word2idx.items():\n",
    "        embedding_matrix[idx] = word2vec.wv[word]\n",
    "\n",
    "    # Convert to PyTorch tensor\n",
    "    embedding_matrix = torch.tensor(embedding_matrix, dtype=torch.float32)\n",
    "\n",
    "\n",
    "\n",
    "    text1_divided = divide_text(text1, sequence_length)\n",
    "    data1 = torch.LongTensor([[word2idx[char] for char in text1_divided[i]] for i in range(len(text1_divided))])\n",
    "\n",
    "\n",
    "    text2_divided = divide_text(text2, sequence_length)\n",
    "    data2 = torch.LongTensor([[word2idx[char] for char in text2_divided[i]] for i in range(len(text2_divided))])\n",
    "\n",
    "\n",
    "    data1_train = data1[:int(training_fraction * data1.shape[0])]\n",
    "    data1_val = data1[int(training_fraction * data1.shape[0]):]\n",
    "\n",
    "    data2_train = data2[:int(training_fraction * data2.shape[0])]\n",
    "    data2_val = data2[int(training_fraction * data2.shape[0]):]\n",
    "\n",
    "\n",
    "    label0_train = torch.zeros(data1_train.shape[0])\n",
    "    label0_val = torch.zeros(data1_val.shape[0])\n",
    "\n",
    "    label1_train = torch.ones(data2_train.shape[0])\n",
    "    label1_val = torch.ones(data2_val.shape[0])\n",
    "\n",
    "\n",
    "    labels_train = torch.cat((label0_train, label1_train), dim = 0)\n",
    "    labels_val = torch.cat((label0_val, label1_val), dim = 0)\n",
    "\n",
    "    data_train = torch.cat((data1_train, data2_train), dim = 0)\n",
    "    data_val = torch.cat((data1_val, data2_val), dim = 0)\n",
    "\n",
    "    data_train = torch.LongTensor(data_train)\n",
    "    labels_train = labels_train.type(torch.LongTensor)\n",
    "    bow_train = BoW(data_train, vocab_size, sequence_length)\n",
    "\n",
    "    dataset_train = TensorDataset(data_train, bow_train, labels_train)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_train = DataLoader(dataset_train, batch_size = batch_size, shuffle=True)\n",
    "    #dataloader_train = DataLoader(dataset_train, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    data_val = torch.LongTensor(data_val)\n",
    "    labels_val = labels_val.type(torch.LongTensor)\n",
    "    bow_val = BoW(data_val, vocab_size, sequence_length)\n",
    "\n",
    "    dataset_val = TensorDataset(data_val, bow_val, labels_val)\n",
    "\n",
    "    # Create a DataLoader with shuffling enabled\n",
    "    dataloader_val = DataLoader(dataset_val, batch_size = batch_size, shuffle = True)\n",
    "    #dataloader_val = DataLoader(dataset_val, batch_size = batch_size)\n",
    "\n",
    "\n",
    "    style0_val = torch.LongTensor(data1_val)\n",
    "    style1_val = torch.LongTensor(data2_val)\n",
    "    \n",
    "    return dataloader_train, dataloader_val, embedding_dim, embedding_matrix, word2vec, idx2word, word2idx, vocab_size, style0_val, style1_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 0])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = torch.tensor([[1,0]])\n",
    "a = a.repeat(32,1)\n",
    "a[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "sequence_length = 25\n",
    "embedding_dim = 300\n",
    "hidden_dim = 256\n",
    "latent_dim = 136"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train loader:  229\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, embedding_dim, embedding_matrix, word2vec, idx2word, word2idx, vocab_size, style0_val, style1_val = custom_dataset('divina_commedia.txt', \n",
    "                                                                                     'uno_nessuno_e_i_malavoglia.txt', \n",
    "                                                                                     sequence_length, \n",
    "                                                                                     embedding_dim,\n",
    "                                                                                     batch_size = 32, \n",
    "                                                                                     training_fraction = 0.9)\n",
    "print('len train loader: ', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len train loader:  356\n"
     ]
    }
   ],
   "source": [
    "train_loader, val_loader, embedding_dim, embedding_matrix, word2vec, idx2word, word2idx, vocab_size, style0_val, style1_val = custom_dataset('divina_commedia.txt', \n",
    "                                                                                     'lo_cunto_de_li_cunti.txt', \n",
    "                                                                                     sequence_length, \n",
    "                                                                                     embedding_dim,\n",
    "                                                                                     batch_size = 32, \n",
    "                                                                                     training_fraction = 0.9)\n",
    "print('len train loader: ', len(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19529"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "sos_token = torch.full((1,),word2idx['<sos>'])\n",
    "sos_token = sos_token.type(torch.LongTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "vae = GRUVAE(embedding_matrix, hidden_dim, latent_dim, vocab_size, sos_token, num_layers = 1)\n",
    "\n",
    "style_classif = StyleClassifier(8)\n",
    "adv_style_classif = AdvStyleClassifier(128)\n",
    "content_classif = ContentClassifier(128, vocab_size)\n",
    "adv_content_classif = AdvContentClassifier(8, vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total parameters:  6590109\n"
     ]
    }
   ],
   "source": [
    "vae_params = sum(p.numel() for p in vae.parameters() if p.requires_grad)\n",
    "style_params = sum(p.numel() for p in style_classif.parameters() if p.requires_grad)\n",
    "style_adv_params = sum(p.numel() for p in adv_style_classif.parameters() if p.requires_grad)\n",
    "content_params = sum(p.numel() for p in content_classif.parameters() if p.requires_grad)\n",
    "adv_content_params = sum(p.numel() for p in adv_content_classif.parameters() if p.requires_grad)\n",
    "total_params =  vae_params + style_params + style_adv_params + content_params + adv_content_params\n",
    "print('Total parameters: ', total_params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 1 Average loss: 0.3179\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 1/5 [02:07<08:29, 127.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 2 Average loss: 0.2058\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 2/5 [04:24<06:39, 133.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 3 Average loss: 0.1551\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 3/5 [06:44<04:32, 136.42s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 4 Average loss: 0.1291\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 4/5 [09:05<02:17, 137.99s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "====> Epoch: 5 Average loss: 0.1124\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 5/5 [11:28<00:00, 137.73s/it]\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "losses = train_VAE(vae,\n",
    "                   style_classif,\n",
    "                   adv_style_classif,\n",
    "                   content_classif,\n",
    "                   adv_content_classif,\n",
    "                   train_loader,\n",
    "                   val_loader,\n",
    "                   num_epochs = 5,\n",
    "                   vocab_size= vocab_size,\n",
    "                   lr = 4e-4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ricostruzione"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stile reale:  Italiano\n",
      "Stile predetto:  0.5954515337944031\n",
      "Input sequence: \n",
      " te resuorve de fare lo consiglio de chi te vo bene all utemo sì sango nuostro e desiderammo l utele e lo gusto tujo però\n",
      "\n",
      "Reconstructed sequence: \n",
      " te move de fare lo consiglio de chi te vo bene all utemo sì sango tujo e tujo l tujo e lo gusto tujo però\n"
     ]
    }
   ],
   "source": [
    "for i ,(data,bow,label) in enumerate(val_loader):\n",
    "    if i == 0:\n",
    "        prova = data[0]\n",
    "        labels = label[0]\n",
    "        boww = bow[0]\n",
    "\n",
    "frase = [idx2word[prova[i].item()] for i in range(prova.shape[0])]\n",
    "\n",
    "prova = prova.view(1,prova.shape[0])\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c  = vae(prova)\n",
    "\n",
    "indices = torch.argmax(reconstructed_sequence, dim=2)\n",
    "\n",
    "ricostruzione = []\n",
    "for i in range(prova.shape[1]):\n",
    "    ricostruzione.append(idx2word[indices[0][i].item()])\n",
    "\n",
    "\n",
    "if labels.item() == 0.0:\n",
    "    stile = 'Dante'\n",
    "else: \n",
    "    stile = 'Italiano'\n",
    "\n",
    "style = style_classif(style)\n",
    "\n",
    "print('Stile reale: ', stile)\n",
    "print('Stile predetto: ', style[0].item())\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruzione))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Style transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Dante to italian: \n",
      "\n",
      "Input sequence: \n",
      " del gran viro a cui nostro segnor lasciò le chiavi ch ei portò giù di questo gaudio miro tenta costui di punti lievi e gravi\n",
      "\n",
      "Reconstructed sequence: \n",
      " del gran contra a cui nostro segnor lasciò le chiavi ch ei portò giù di questo ricominciò render tenta costui di spesse uomini e gravi\n",
      "\n",
      "Transferred sequence: \n",
      " del gran peluso a cui nostro segnor lasciò le chiavi ch ei portò giù di questo ricominciò manto tenta costui di spesse uomini e gravi\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c  = vae(style1_val)\n",
    "\n",
    "\n",
    "#style1 = z_s.mean(dim=1).view(z_s.shape[0],1,z_s.shape[2])\n",
    "style1 = style.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "n = 25\n",
    "\n",
    "with torch.no_grad():    \n",
    "    transfer_output = vae.style_transfer(style0_val[n].view(1,style0_val.shape[1]), style1)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    reconstructed, style, content, mu_s, logvar_s, mu_c, logvar_c  = vae(style0_val[n].view(1,style0_val.shape[1]))\n",
    "\n",
    "\n",
    "index = torch.argmax(reconstructed,dim=2)\n",
    "\n",
    "indices = torch.argmax(transfer_output, dim=2)\n",
    "\n",
    "frase_trasferita = []\n",
    "\n",
    "for i in range(transfer_output.shape[1]):\n",
    "    frase_trasferita.append(idx2word[indices[0][i].item()])\n",
    "\n",
    "ricostruita = []\n",
    "for i in range(transfer_output.shape[1]):\n",
    "    ricostruita.append(idx2word[index[0][i].item()])\n",
    "\n",
    "frase = [idx2word[style0_val[n][i].item()] for i in range(style0_val.shape[1])]\n",
    "\n",
    "\n",
    "print(\"From Dante to italian: \\n\")\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(ricostruita))\n",
    "print(\"\\nTransferred sequence: \\n\", ' '.join(frase_trasferita))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "From Italian to Dante: \n",
      "\n",
      "Input sequence: \n",
      " soffiò loro lo speziale facendo gli occhiacci volete che andiamo tutti in galera sappiate che colla giustizia bisogna dir sempre di no e che noi\n",
      "\n",
      "Reconstructed sequence: \n",
      " alle loro lo speziale facendo gli gridò volete che andiamo tutti in casa stata che colla giustizia bisogna dir sempre di no e che noi\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    reconstructed_sequence, style, content, mu_s, logvar_s, mu_c, logvar_c  = vae(style0_val)\n",
    "\n",
    "\n",
    "#style1 = z_s.mean(dim=1).view(z_s.shape[0],1,z_s.shape[2])\n",
    "style0 = style.mean(dim=1).unsqueeze(1)\n",
    "\n",
    "n = 55\n",
    "\n",
    "with torch.no_grad():    \n",
    "    transfer_output = vae.style_transfer(style1_val[n].view(1,style1_val.shape[1]), style0)\n",
    "\n",
    "\n",
    "indices = torch.argmax(transfer_output, dim=2)\n",
    "\n",
    "frase_trasferita = []\n",
    "\n",
    "for i in range(transfer_output.shape[1]):\n",
    "    frase_trasferita.append(idx2word[indices[0][i].item()])\n",
    "\n",
    "\n",
    "frase = [idx2word[style1_val[n][i].item()] for i in range(style1_val.shape[1])]\n",
    "\n",
    "\n",
    "print(\"From Italian to Dante: \\n\")\n",
    "print(\"Input sequence: \\n\", ' '.join(frase))\n",
    "print(\"\\nReconstructed sequence: \\n\", ' '.join(frase_trasferita))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ox",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
